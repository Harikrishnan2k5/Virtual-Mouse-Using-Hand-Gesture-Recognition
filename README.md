ğŸ–±ï¸ Virtual Mouse Using Hand Gesture Recognition (Click Control)
ğŸ“˜ Overview

This project demonstrates how a webcam can be used to control mouse movement and perform click actions using simple hand gestures.
It uses computer vision and machine learning-based landmark detection (MediaPipe) to track finger movements and translate them into cursor and click operations â€” without needing a physical mouse.

ğŸš€ Features

âœ… Real-time cursor movement using index finger.
âœ… Left-click action when the thumb and index finger come close together.
âœ… Fully works with just a webcam â€” no sensors required.
âœ… Lightweight and efficient; runs on CPU.
âœ… Simple, intuitive, and interactive humanâ€“computer interface.

ğŸ§  Technologies Used
Library	Purpose
OpenCV	Captures real-time video frames and processes them for gesture detection.
MediaPipe	Detects and tracks 21 hand landmarks using machine learning models.
PyAutoGUI	Simulates mouse movement and click operations on the screen.
Math (Python)	Used to calculate the Euclidean distance between finger coordinates.
âš™ï¸ How It Works

Camera Capture (OpenCV)
The webcam captures live frames and mirrors them for natural interaction.

Hand Landmark Detection (MediaPipe)
MediaPipeâ€™s pre-trained ML model identifies 21 key points of the hand.

Cursor Movement
The index fingerâ€™s position is mapped to the screen coordinates using proportional scaling.

Click Detection
The distance between the thumb tip (Landmark 4) and index finger tip (Landmark 8) is calculated.

If the distance is less than a defined threshold â†’ click action is triggered using pyautogui.click().

Visual Feedback
Circles are drawn on the fingertips to show tracking accuracy.

ğŸ§© Code Structure
VirtualMouse/
â”‚
â”œâ”€â”€ code.py             # Main project file
â”œâ”€â”€ requirements.txt    # Library dependencies
â””â”€â”€ README.md           # Project documentation

ğŸ§‘â€ğŸ’» Developed By

Hari Krishnan M
B.Tech in Computer Science and Engineering
Lovely Professional University, Jalandhar, India
ğŸ“§ hari16krishna2k5@gmail.com

ğŸ‘©â€ğŸ« Guided by: Ms. Deepali Kumari, Assistant Professor

ğŸ§  Core Concept: Where ML is Used

The MediaPipe Hands API uses a pre-trained deep learning model to:

Detect hands in the camera feed.

Identify 21 hand landmarks (like fingertips, joints, etc.) in real-time.

Provide landmark coordinates (x, y, z), which are used for gesture recognition.

So even though you didnâ€™t train the ML model yourself, youâ€™re using Googleâ€™s ML-based detection engine in your project â€” thatâ€™s where the machine learning part is applied!

ğŸ Future Enhancements

ğŸ’¡ Add right-click and scroll gestures.
ğŸ¯ Smoothen pointer movement with Kalman filtering.
ğŸš€ Enable multiple-hand gesture support for more interactions.
